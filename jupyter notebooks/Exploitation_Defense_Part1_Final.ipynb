{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🧠 Defense & Exploitation of Large Language Models (LLMs)\n",
        "\n",
        "Welcome to this hands-on companion notebook for the course **\"Defense and Exploitation of LLMs\"**. Throughout this tutorial, we will explore how Large Language Models (LLMs) can be attacked, manipulated, and defended through practical techniques using the a variety of transformer models (GPT-2, Phi-2, Phi-4, etc.). Some of these models are light and performant — ideal for Colab free tier, while others are larger and require extra computer (GPUs) like the A100 on the Pro version of Colab. We also will show you throughout the course how to utilize these models __offline__ for your own research and development in air-gapped networks as you all normally work. Please run the notebook cells that best pertain to your situation. This notebook doesn't fully utilize our methodology for loading models but Part 2 and Part 3 will adequately set up a pipeline."
      ],
      "metadata": {
        "id": "KYQCScXGL8CQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22yxV-9O59jw"
      },
      "outputs": [],
      "source": [
        "# run this codeblock for both groups (Colab or Local)\n",
        "# local notebooks may need to wait until their environment is fully set up first instructions below\n",
        "# pip installs are not needed in the Colab environment unless asked specifically\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModel, GPT2Tokenizer, BertTokenizer, BertModel, T5Tokenizer, GPT2LMHeadModel, DistilBertTokenizer, DistilBertModel\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.dpi'] = 72\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from IPython.display import Markdown\n",
        "import warnings\n",
        "transformers.logging.set_verbosity_error()\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "# Set seaborn style for nicer plots\n",
        "sns.set(style='whitegrid')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For Colab Users\n",
        "## 💻 Setup and Model Loading\n",
        "\n",
        "To authenticate with the Hugging Face for using an LLM:\n",
        "- Login to the HuggingFace, Goto https://huggingface.co/settings/tokens\n",
        "- Click on \"+ Create new token\"\n",
        "- Select Token type as \"Write\", Give a name as \"responsible_ai\" and create the token\n",
        "- Copy the `HF_TOKEN` displayed to you, it would look something like \"hf_MQndTFAzdVxxxxxxxxxxxxxxxctLtWoIoaMabO\"\n",
        "- Open the Secrets in your Google Colab, give the Name = `HF_TOKEN` and Value = `hf_MQndTFAzdVxxxxxxxxxxxxxxxctLtWoIoaMabO`\n",
        "- Turn ON the Notebook access."
      ],
      "metadata": {
        "id": "H5tbcWYMM8rD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For Local Python Users  \n",
        "## 💻 Setup and Model Loading Instructions\n",
        "\n",
        "### 1. Install Required Packages  \n",
        "Make sure you have the `transformers` and `torch` libraries installed:\n",
        "\n",
        "```bash\n",
        "pip install transformers torch\n",
        "```\n",
        "To authenticate with the Hugging Face for using an LLM:\n",
        "- Login to the HuggingFace, Goto https://huggingface.co/settings/tokens\n",
        "- Click on \"+ Create new token\"\n",
        "- Select Token type as \"Write\", Give a name as \"responsible_ai\" and create the token\n",
        "- Copy the `HF_TOKEN` displayed to you, it would look something like \"hf_MQndTFAzdVxxxxxxxxxxxxxxxctLtWoIoaMabO\"\n",
        "\n",
        "```python\n",
        "os.environ[\"HF_TOKEN\"] = \"hf_your_actual_token_here\"\n",
        "```"
      ],
      "metadata": {
        "id": "m0dDbEzsNn69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Your Environment"
      ],
      "metadata": {
        "id": "RGxrxk7vOb0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'PyTorch version= {torch.__version__}')\n",
        "print(f'transformers version= {transformers.__version__}')\n",
        "print(f'CUDA available= {torch.cuda.is_available()}')\n",
        "Device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "ssQBim2VObPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large Language Models (LLMs) Basics\n",
        "This notebook provides an introductory overview of how large language models work.\n",
        "\n",
        "We will cover:\n",
        "- What is an LLM?\n",
        "- Tokenization basics\n",
        "- Deep learning and embeddings\n",
        "- Transformer architecture and self-attention\n",
        "- How vectors become words\n",
        "- Attention visualization example"
      ],
      "metadata": {
        "id": "OvO83FxE6Bwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is a Large Language Model?\n",
        "Large language models (LLMs) are deep neural networks trained on vast amounts of text data to understand and generate human-like language. They learn statistical patterns of language, such as grammar, facts, and context.\n",
        "\n",
        "LLMs work by converting text into numeric vectors, processing these vectors through many layers (typically Transformers), and then generating output text token-by-token.\n",
        "\n"
      ],
      "metadata": {
        "id": "2fk2G3xx6U_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Tokenization\n",
        "Tokenization breaks text into smaller units (tokens), which may be words, subwords, or characters.\n",
        "\n",
        "This notebook demonstrates different tokenization methods used in NLP and LLMs:\n",
        "\n",
        "- Word-level tokenization (simple split)\n",
        "- Byte Pair Encoding (BPE) via GPT-2 tokenizer\n",
        "- WordPiece tokenization via BERT tokenizer\n",
        "- SentencePiece tokenization via T5 tokenizer\n",
        "- Character-level tokenization\n",
        "\n",
        "We apply each to the same example sentence to compare outputs.\n"
      ],
      "metadata": {
        "id": "SlS3Umuj62Hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I do not like this movie.\"\n",
        "\n",
        "print(f\"Input sentence:\\n{sentence}\\n\")\n",
        "\n",
        "# 1. Word-level tokenization (simple split)\n",
        "word_tokens = sentence.split()\n",
        "print(\"Word-level tokens:\")\n",
        "print(word_tokens)\n",
        "print()\n",
        "\n",
        "# 2. BPE tokenization (GPT-2 tokenizer)\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_tokens = gpt2_tokenizer.tokenize(sentence)\n",
        "print(\"BPE tokens (GPT-2):\")\n",
        "print(gpt2_tokens)\n",
        "print()\n",
        "\n",
        "# 3. WordPiece tokenization (BERT tokenizer)\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_tokens = bert_tokenizer.tokenize(sentence)\n",
        "print(\"WordPiece tokens (BERT):\")\n",
        "print(bert_tokens)\n",
        "print()\n",
        "\n",
        "# 4. SentencePiece tokenization (T5 tokenizer)\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "t5_tokens = t5_tokenizer.tokenize(sentence)\n",
        "print(\"SentencePiece tokens (T5):\")\n",
        "print(t5_tokens)\n",
        "print()\n",
        "\n",
        "# 5. Character-level tokenization (manual)\n",
        "char_tokens = list(sentence)\n",
        "print(\"Character-level tokens:\")\n",
        "print(char_tokens)\n",
        "print()\n"
      ],
      "metadata": {
        "id": "TFXtjCGo7AaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning & Embeddings: Representing Tokens as Vectors\n",
        "\n",
        "Tokens are converted into dense numerical vectors called **embeddings**.  \n",
        "Embeddings capture semantic meaning — similar words have similar vectors.\n",
        "\n",
        "These vectors are the inputs to the neural network model.\n",
        "\n",
        "Let's see how to get embeddings from a pretrained model.\n"
      ],
      "metadata": {
        "id": "s0rbjGa27zYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "model = DistilBertModel.from_pretrained(model_name)\n",
        "\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "print(f\"Tokenized input IDS: {inputs['input_ids']}\")\n",
        "print(f\"Tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")\n",
        "print()\n",
        "\n",
        "with torch.no_grad():\n",
        "    embeddings = model.embeddings.word_embeddings(inputs[\"input_ids\"])\n",
        "\n",
        "print(f\"Embedding tensor shape:\", embeddings.shape)  # (batch_size, seq_len, embedding_dim)\n",
        "print(f\"Each token is represented by a {embeddings.shape[-1]}-dimensional vector.\\n\")\n",
        "\n",
        "# Show embedding for a specific token (index 2 should be 'do')\n",
        "token_index = 2\n",
        "token_id = inputs['input_ids'][0][token_index].item()\n",
        "token_str = tokenizer.convert_ids_to_tokens([token_id])[0]\n",
        "\n",
        "print(f\"Embedding vector for token '{token_str}' (first 10 dimensions):\")\n",
        "print(embeddings[0, token_index, :10])\n",
        "print(\"...\")\n",
        "print(f\"Full embedding has {embeddings.shape[-1]} dimensions\")\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HKWnJTwc7y4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Architecture Overview: The Heart of Modern LLMs\n",
        "\n",
        "Transformers are the core architecture behind most LLMs.\n",
        "\n",
        "Key ideas:\n",
        "\n",
        "- **Self-Attention:** Each token looks at other tokens to understand context.\n",
        "- **Feed-forward layers:** Further process the attention outputs.\n",
        "- **Stacking layers:** Multiple layers enable learning complex language features.\n",
        "\n",
        "The self-attention mechanism allows the model to weigh the importance of each token relative to others in the sequence.\n",
        "\n",
        "## Attention Mechanism in Detail & Visualization\n",
        "\n",
        "Attention computes scores between tokens, telling the model which tokens to focus on.\n",
        "\n",
        "Let's visualize attention weights from the last layer of the DistilBERT model for our sentence.\n",
        "\n",
        "We average attention across all heads for simplicity.\n"
      ],
      "metadata": {
        "id": "wG80sMXg8Ja3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get attention weights by running the model with output_attentions=True\n",
        "with torch.no_grad():\n",
        "    # Only pass the inputs that DistilBERT expects\n",
        "    model_inputs = {\n",
        "        'input_ids': inputs['input_ids'],\n",
        "        'attention_mask': inputs['attention_mask']\n",
        "    }\n",
        "    outputs = model(**model_inputs, output_attentions=True)\n",
        "\n",
        "# Get attention from the last layer\n",
        "attentions = outputs.attentions[-1]  # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
        "print(f\"Attention tensor shape: {attentions.shape}\")\n",
        "print(f\"Number of attention heads: {attentions.shape[1]}\")\n",
        "\n",
        "# Average attention across all heads\n",
        "avg_attention = attentions[0].mean(dim=0).cpu().numpy()\n",
        "\n",
        "# Get the tokens for labeling\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "print(f\"Tokens: {tokens}\\n\")\n",
        "\n",
        "# Create attention heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    avg_attention,\n",
        "    xticklabels=tokens,\n",
        "    yticklabels=tokens,\n",
        "    cmap=\"viridis\",\n",
        "    annot=True,\n",
        "    fmt='.3f',\n",
        "    cbar_kws={'label': 'Attention Weight'}\n",
        ")\n",
        "plt.title(\"Average Attention Weights (Last Layer)\\nEach cell shows how much the row token attends to the column token\")\n",
        "plt.xlabel(\"Key Tokens (what we attend TO)\")\n",
        "plt.ylabel(\"Query Tokens (what is attending)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show some specific attention patterns\n",
        "print(\"\\nInteresting attention patterns:\")\n",
        "for i, query_token in enumerate(tokens):\n",
        "    if query_token not in ['[CLS]', '[SEP]']:\n",
        "        max_attention_idx = np.argmax(avg_attention[i])\n",
        "        max_attention_token = tokens[max_attention_idx]\n",
        "        max_attention_score = avg_attention[i, max_attention_idx]\n",
        "        print(f\"'{query_token}' attends most to '{max_attention_token}' with weight {max_attention_score:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "XPa92xKh8TRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoding: From Vectors Back to Words\n",
        "\n",
        "After processing the input, the model predicts the next token by generating a probability distribution over its vocabulary.\n",
        "\n",
        "It picks the token with the highest probability (or samples probabilistically), converts its ID back to a word piece, and generates text token-by-token.\n",
        "\n",
        "This iterative decoding enables text generation, translation, summarization, and more.\n"
      ],
      "metadata": {
        "id": "EVeP6woH8b1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demonstrating the Full Prediction Cycle of an LLM\n",
        "\n",
        "In this example, we show how a large language model predicts the next word (token) given a text prompt.\n",
        "\n",
        "**What happens step-by-step:**\n",
        "\n",
        "1. **Input Prompt:**  \n",
        "   We start with a human-readable text input, e.g., `\"The capital of Virginia is\"`.  \n",
        "\n",
        "2. **Tokenization:**  \n",
        "   The prompt is broken down into tokens using the model’s tokenizer. These tokens are converted into numeric IDs that the model can process.\n",
        "\n",
        "3. **Model Forward Pass:**  \n",
        "   The tokens are fed into the pretrained language model, which outputs **logits** — raw scores representing how likely each token in the vocabulary is to come next.\n",
        "\n",
        "4. **Next-Token Prediction:**  \n",
        "   We extract the logits corresponding to the position after the last input token, then apply a softmax function to convert these scores into probabilities.\n",
        "\n",
        "5. **Top Candidate Tokens:**  \n",
        "   The model ranks tokens by their probability of being the next token and we display the top choices with their associated likelihoods.\n",
        "\n",
        "**Why this represents the full cycle:**\n",
        "\n",
        "- The LLM does not “understand” facts or intentions internally; it **predicts the next token** solely based on learned statistical patterns from massive text data.  \n",
        "- This next-token prediction is repeated token-by-token to generate fluent text, which is how models produce sentences, paragraphs, and even long documents.  \n",
        "- By seeing the actual probabilities, we gain insight into the model’s “thought process” — which next words it considers most plausible given the prompt context.\n",
        "\n",
        "This demonstration ties together all components covered so far: tokenization, embeddings (hidden inside the model), Transformer processing, and decoding predictions back into human-readable words.\n",
        "\n",
        "It’s a hands-on view of what really happens inside an LLM when it generates language.\n"
      ],
      "metadata": {
        "id": "GntDa3Or8_fA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model (GPT-2 small for speed)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"The capital of Virginia is\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Forward pass to get logits\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "# Get logits for the last token's next token prediction\n",
        "last_token_logits = logits[0, -1, :]\n",
        "\n",
        "# Convert logits to probabilities (softmax)\n",
        "probs = torch.softmax(last_token_logits, dim=-1)\n",
        "\n",
        "# Get top 10 tokens with highest probability\n",
        "top_probs, top_indices = torch.topk(probs, k=10)\n",
        "\n",
        "print(f\"Prompt: {prompt}\\n\")\n",
        "print(\"Top next token predictions with probabilities:\")\n",
        "for token_id, prob in zip(top_indices, top_probs):\n",
        "    token_str = tokenizer.decode([token_id])\n",
        "    print(f\"{token_str.strip()}: {prob.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "2kVGa-aq8zBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GPT-2 Medium model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"The capital of Virginia is\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Forward pass to get logits\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "# Get logits for the last token's next token prediction\n",
        "last_token_logits = logits[0, -1, :]\n",
        "\n",
        "# Convert logits to probabilities (softmax)\n",
        "probs = torch.softmax(last_token_logits, dim=-1)\n",
        "\n",
        "# Get top 10 tokens with highest probability\n",
        "top_probs, top_indices = torch.topk(probs, k=10)\n",
        "\n",
        "print(f\"Prompt: {prompt}\\n\")\n",
        "print(\"Top next token predictions with probabilities:\")\n",
        "for token_id, prob in zip(top_indices, top_probs):\n",
        "    token_str = tokenizer.decode([token_id])\n",
        "    print(f\"{token_str.strip()}: {prob.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "gOjlcPv1LiX0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}