{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ddf74e05-53b7-442c-80e1-c52a8ee85491",
      "metadata": {
        "id": "ddf74e05-53b7-442c-80e1-c52a8ee85491"
      },
      "source": [
        "<div align=\"right\">June 2025 | &copy; JHU </div>\n",
        "\n",
        "<img src=\"./figs/ep.logo.png\" alt=\"JHU EP logo\" width=\"200\" align=\"right\" />\n",
        "\n",
        "# Retrieval-Augmented Generation\n",
        "In this example, we will build and test a simple Retrieval-Augmented Generation with Large Language Models (RAG-LLM). It typically has two main components:\n",
        "- Retriever identifies relevant context or documents from an external data source. It often uses embeddings to find and rank content based on similarity to the user's query. Popular retrievers include dense retrievers like Sentence Transformers, FAISS, or BM25-based systems. The retriever allows RAG-LLM to pull in precise and contextual information to aid the generative model in crafting accurate responses.\n",
        "- Generators usually use a large language model, such as GPT or T5, which takes the retrieved context and the user's query as input and generates a coherent, contextually relevant answer. The generator relies on the retriever-provided context to produce detailed, accurate answers to specific questions.\n",
        "\n",
        "The example below includes,\n",
        "- The external data source is from the JSON file `squad-train-v2.0.json`, which is part of the SQuAD (Stanford Question Answering Dataset) v2.0, a popular dataset for training and evaluating machine learning models on reading comprehension and question-answering tasks. Stanford released this dataset and is widely used in natural language processing (NLP) for benchmarking question-answering systems (source: &#128214; **<a href=\"https://rajpurkar.github.io/SQuAD-explorer/\" target=\"_blank\">https://rajpurkar.github.io/SQuAD-explorer/</a>**)\n",
        "- Retriever is a `Sentence Transformer` (source: &#129303; **<a href=\"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\" target=\"_blank\">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</a>**)\n",
        "- Large Language Model (LLM) is a `Llama-3.2-1B-Instruct` model (source: &#9854;&#65039; **<a href=\"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\" target=\"_blank\">https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct</a>**)\n",
        "\n",
        "Install the sentence transformer: `pip install sentence-transformers`&#8617;\n",
        "\n",
        "Meta requires permission to download the model, which requires an `edu` email address. As in the previous lectures, follow the instructions to request permission.\n",
        "\n",
        "Since the LLM we utilize in this example, `meta-llama/Llama-3.2-1B-Instruct`, can process the entire context, we will store only two key pieces of information: topics and contexts. At this time, we will not process `qas` section (question-answer section)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb7f896f-9b9b-4fa4-a45b-368d35c9f465",
      "metadata": {
        "id": "fb7f896f-9b9b-4fa4-a45b-368d35c9f465"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.dpi'] = 72\n",
        "from IPython.display import Markdown\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "MODEL_PATH= '/EP_models/'\n",
        "os.environ['HF_HOME'] = MODEL_PATH  # before import transformers\n",
        "os.environ['HF_DATASETS_OFFLINE']= '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b734aa94-10f8-4186-9477-52474176dadc",
      "metadata": {
        "id": "b734aa94-10f8-4186-9477-52474176dadc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# filter warnings\n",
        "import warnings\n",
        "transformers.logging.set_verbosity_error()\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "\n",
        "Device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f'PyTorch version= {torch.__version__}')\n",
        "print(f'transformers version= {transformers.__version__}')\n",
        "print(f'CUDA available= {torch.cuda.is_available()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4bd5d62-6ae8-4091-a566-39f75b7284d5",
      "metadata": {
        "id": "d4bd5d62-6ae8-4091-a566-39f75b7284d5"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import login\n",
        "# login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38c9e7a2-8af6-41d6-9873-9d3d1bc2c017",
      "metadata": {
        "id": "38c9e7a2-8af6-41d6-9873-9d3d1bc2c017"
      },
      "outputs": [],
      "source": [
        "# explore SQuAD training data structure\n",
        "with open('/EP_datasets/squad-train-v2.0.json', 'r') as f:\n",
        "    squad_data = json.load(f)\n",
        "\n",
        "n_titles = len(squad_data['data'])\n",
        "print(f'There are {n_titles} titles/topics in SQuAD v2.0 training dataset.\\n')\n",
        "\n",
        "title_idx = 200\n",
        "prg_idx = 2\n",
        "current_title = squad_data['data'][title_idx]['title']\n",
        "n_paragraphs = len(squad_data['data'][title_idx]['paragraphs'])\n",
        "n_qas = len(squad_data['data'][title_idx]['paragraphs'][prg_idx]['qas'])\n",
        "print(f\"Title with index #{title_idx} is {squad_data['data'][title_idx]['title']}.\")\n",
        "print(f\"Title {current_title} has {n_paragraphs} paragraphs. Paragraph with index {prg_idx} has {n_qas} question-answer pairs.\")\n",
        "\n",
        "# docs = [entry['context'] for entry in squad_data['data'][0]['paragraphs']]\n",
        "# squad_data['data']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5428651b-3cf5-4b84-98d5-3ca123c6ac4e",
      "metadata": {
        "id": "5428651b-3cf5-4b84-98d5-3ca123c6ac4e"
      },
      "outputs": [],
      "source": [
        "# squad_data['data'][200]['paragraphs'][0]['qas']\n",
        "squad_data['data'][200]['paragraphs'][0]['context']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccf6e82b-47e6-4c88-8efa-5014ab7ff49e",
      "metadata": {
        "id": "ccf6e82b-47e6-4c88-8efa-5014ab7ff49e"
      },
      "outputs": [],
      "source": [
        "sqdata = {\"topic\": [], \"context\": []}\n",
        "\n",
        "for i in range(n_titles):\n",
        "    n_paragraphs = len(squad_data['data'][i]['paragraphs'])\n",
        "    for j in range(n_paragraphs):\n",
        "        sqdata[\"topic\"].append(squad_data['data'][i]['title'])\n",
        "        sqdata[\"context\"].append(squad_data['data'][i]['paragraphs'][j]['context'])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(sqdata)\n",
        "\n",
        "# sanity\n",
        "print(f'Shape of df: {df.shape}')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08b54619-189d-4b50-9919-e2863ad11845",
      "metadata": {
        "id": "08b54619-189d-4b50-9919-e2863ad11845"
      },
      "outputs": [],
      "source": [
        "class RAGLLM:\n",
        "    def __init__(self, external_data):\n",
        "        self.st_model = SentenceTransformer('all-MiniLM-L6-v2')  # sentence transformer is used for embeddings\n",
        "\n",
        "        # external data\n",
        "        self.external_data = external_data\n",
        "        self.titles = self.external_data['topic'].unique()\n",
        "        self.title_embeddings = self.st_model.encode(self.titles.tolist())  # embed each topic/title\n",
        "\n",
        "        # LLM model\n",
        "        self.generator = pipeline('text-generation', model='meta-llama/Llama-3.2-1B-Instruct',\n",
        "                                  max_length=4096, device=Device)  # utilize GPU in this example\n",
        "\n",
        "    def generate_answer(self, user_question):\n",
        "        # Step 1: Find the best matching topic\n",
        "        question_embedding = self.st_model.encode([user_question])\n",
        "        title_similarities = cosine_similarity(question_embedding, self.title_embeddings)\n",
        "        best_title_index = np.argmax(title_similarities)\n",
        "        best_title = self.titles[best_title_index]\n",
        "        print(f\"The best topic: {best_title}\\n\")\n",
        "\n",
        "        # Step 2: Filter contexts for the identified best topic\n",
        "        topic_contexts = self.external_data[self.external_data['topic'] == best_title]['context'].tolist()\n",
        "\n",
        "        # Step 3: Vectorize each context under the best topic and find the best matching context\n",
        "        context_embeddings = self.st_model.encode(topic_contexts)\n",
        "        context_similarities = cosine_similarity(question_embedding, context_embeddings)\n",
        "        best_context_index = np.argmax(context_similarities)\n",
        "        best_context = topic_contexts[best_context_index]\n",
        "        print(f\"The best paragraph: {best_context}\\n\")\n",
        "\n",
        "        # Step 4: Generate the answer using LLM with retrieved context\n",
        "        prompt = f\"Context: {best_context}\\n\\nQuestion: {user_question}\\nAnswer:\"\n",
        "        response = self.generator(prompt, max_length=300)\n",
        "        answer_text = response[0]['generated_text']\n",
        "        answer = answer_text.split(\"Answer: \")[-1]\n",
        "        return answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6bbd8cf-3da2-4596-a496-bd508307b65b",
      "metadata": {
        "id": "e6bbd8cf-3da2-4596-a496-bd508307b65b"
      },
      "outputs": [],
      "source": [
        "query = \"Who is the US president in 2012?\"\n",
        "ragllm = RAGLLM(df)\n",
        "output = ragllm.generate_answer(query)\n",
        "for part in output.split(\"\\n\\n\"):\n",
        "    print(part)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6ac36d5-d710-4f95-adf7-252d99a62f71",
      "metadata": {
        "id": "d6ac36d5-d710-4f95-adf7-252d99a62f71"
      },
      "source": [
        "<div style=\"background-color: #F1E0D6; padding: 10px; border-radius: 5px;\">\n",
        "Analysis: The output is correct. The best topic from the external data source appears to be relevant, although the year in the best topic is 2004, while the question asks about 2012. The best paragraph mentions only President Bush, but the answer is Barack Obama which is correct.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b503a05-a973-46ea-9677-03eafccd95a7",
      "metadata": {
        "id": "3b503a05-a973-46ea-9677-03eafccd95a7"
      },
      "outputs": [],
      "source": [
        "query = \"What is the largest state in USA by area?\"\n",
        "ragllm = RAGLLM(df)\n",
        "output = ragllm.generate_answer(query)\n",
        "for part in output.split(\"\\n\\n\"):\n",
        "    print(part)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b88fa98a-8a25-4e3c-a797-9795c4fac9df",
      "metadata": {
        "id": "b88fa98a-8a25-4e3c-a797-9795c4fac9df"
      },
      "source": [
        "<div style=\"background-color: #F1E0D6; padding: 10px; border-radius: 5px;\">\n",
        "Analysis: The output is incorrect. The best topic from the external data source appears to be relevant. However, the best paragraph is quite off. It is also possible that the chosen topic wasn't the most relevant one. From the paragraph, we can see that the geography of the US basically discusses the geography of the US as a country, not its individual states. The information about the largest state in the US by area is possibly in the Alaska topic.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "352e8152-b320-4625-9fcc-dc0d1d9248c1",
      "metadata": {
        "id": "352e8152-b320-4625-9fcc-dc0d1d9248c1"
      },
      "outputs": [],
      "source": [
        "query = \"Which state is larger by area in the USA: Alaska or Texas?\"\n",
        "ragllm = RAGLLM(df)\n",
        "output = ragllm.generate_answer(query)\n",
        "for part in output.split(\"\\n\\n\"):\n",
        "    print(part)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "052c4487-7775-4968-ae3a-b299f9b1fd14",
      "metadata": {
        "id": "052c4487-7775-4968-ae3a-b299f9b1fd14"
      },
      "source": [
        "<div style=\"background-color: #F1E0D6; padding: 10px; border-radius: 5px;\">\n",
        "Analysis: The output is correct. Both the best topic and the best paragraph from the external data source are on point.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b69891-4565-4a97-9792-6696c46aa9cf",
      "metadata": {
        "id": "57b69891-4565-4a97-9792-6696c46aa9cf"
      },
      "outputs": [],
      "source": [
        "query = \"What is the population of Florida?\"\n",
        "ragllm = RAGLLM(df)\n",
        "output = ragllm.generate_answer(query)\n",
        "for part in output.split(\"\\n\\n\"):\n",
        "    print(part)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bede8f3-44fa-460f-8901-bc7b51a77e9f",
      "metadata": {
        "id": "2bede8f3-44fa-460f-8901-bc7b51a77e9f"
      },
      "source": [
        "<div style=\"background-color: #F1E0D6; padding: 10px; border-radius: 5px;\">\n",
        "Analysis: The output is correct. This question is quite general, as it does not mention a specific timeline. Therefore, the answer depends on how recent the external data source is. The last update to the SQuAD 2.0 dataset was in 2018. Hence, it makes sense that the output returned the population of Florida in 2015. Both the best topic and the best paragraph are relevant to the question.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5c0f2c7-626c-4c03-9e00-404d4ce24739",
      "metadata": {
        "id": "c5c0f2c7-626c-4c03-9e00-404d4ce24739"
      },
      "outputs": [],
      "source": [
        "query = \"Where is Mount Rushmore?\"\n",
        "ragllm = RAGLLM(df)\n",
        "output = ragllm.generate_answer(query)\n",
        "for part in output.split(\"\\n\\n\"):\n",
        "    print(part)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6471e69d-be38-41e2-bfb6-ca5fa0786a97",
      "metadata": {
        "id": "6471e69d-be38-41e2-bfb6-ca5fa0786a97"
      },
      "source": [
        "<div style=\"background-color: #F1E0D6; padding: 10px; border-radius: 5px;\">\n",
        "Analysis: The output is correct. However, the best topic and the best paragraph in the external dataset are not relevant.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3a91395-a7f6-422a-ba34-9259b733b906",
      "metadata": {
        "id": "a3a91395-a7f6-422a-ba34-9259b733b906"
      },
      "source": [
        "## Analysis and Observations\n",
        "The main purpose of the example above is to demonstrate a simple implementation of the RAG with an LLM. While this approach is useful, there are areas for improvement. Key points include:\n",
        "1. External Dataset Update: The SQuAD 2.0 training dataset was last updated in 2018, while the Llama-3.2-1B-Instruct model was last updated in September 2024. As a result, the pre-trained LLM contains more recent information and data.\n",
        "2. Dataset Limitations: The external dataset SQuAD 2.0 training dataset may have limitations that can cause the LLM to rely on pre-trained data instead of the external dataset for some outputs. This reflects the constraints of relying on solely static external sources.\n",
        "3. Efficiency of Updates: In practice, updating external datasets is more efficient and optimized compared to updating the entire pre-trained data in an LLM. This efficiency contributes to the growing popularity and practicality of RAG-LLMs.\n",
        "***\n",
        "## Why RAG-LLM Is Better Than LLM Alone?\n",
        "Using RAG-LLM is often better than using an LLM alone because RAG combines the strengths of both retrieval-based systems and generative language models. Here's why it can be advantageous:\n",
        "1. Up-to-Date Knowledge: Accesses external, real-time data; LLMs rely on static, pre-training knowledge.\n",
        "2. Improved Accuracy: Grounds responses in retrieved documents, reducing hallucinations.\n",
        "3. Domain-Specific Flexibility: Tailored to specific domains by indexing relevant data.\n",
        "4. Reduced Training Costs: Updates only the retrieval database, avoiding LLM retraining.\n",
        "5. Transparency: Provides references or sources for answers, enhancing trust.\n",
        "6. Cost Efficiency: Uses smaller models with retrieval for similar or better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "115b7709-eb3f-4bbb-b825-d857ae65e20a",
      "metadata": {
        "id": "115b7709-eb3f-4bbb-b825-d857ae65e20a"
      },
      "source": [
        "<div style=\"background-color: #F1E0D6; padding: 10px; border-radius: 5px;\">\n",
        "In the example below, we will illustrate Domain-Specific Flexibility. In the SQuAD dataset, under the 'Florida' topic, 'Florida' is mentioned with its meaning as 'flower' in Spanish. This information is quite specific and possibly unique. However, the pre-trained dataset of LLaMA may be more general. Therefore, while an LLM may not be able to answer the question correctly, a RAG-LLM will be able to do so in the Explanation part.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f71b27dd-c8cc-4818-8257-f1adf79333f2",
      "metadata": {
        "id": "f71b27dd-c8cc-4818-8257-f1adf79333f2"
      },
      "outputs": [],
      "source": [
        "class LLaMAQA:\n",
        "    def __init__(self, model_name='meta-llama/Llama-3.2-1B-Instruct'):\n",
        "        self.model_name = model_name\n",
        "        print(f\"Loading model '{model_name}'...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    def ask_question(self, question, max_length=50, temperature=0.7, top_p=0.9, top_k=50):\n",
        "        \"\"\"\n",
        "        Ask a question and get a response from the model.\n",
        "        Parameters:\n",
        "            question (str): The question to ask.\n",
        "            max_length (int): Maximum length of the response.\n",
        "            temperature (float): Sampling temperature for randomness.\n",
        "            top_p (float): Nucleus sampling parameter.\n",
        "            top_k (int): Top-k sampling parameter.\n",
        "        Returns:\n",
        "            str: The model's response.\n",
        "        \"\"\"\n",
        "        inputs = self.tokenizer(question, return_tensors=\"pt\")\n",
        "\n",
        "        # generate the response\n",
        "        outputs = self.model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            do_sample=True\n",
        "        )\n",
        "        # decode and return the response\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9947f3b2-1e03-4e4e-a720-3e66645fabf5",
      "metadata": {
        "id": "9947f3b2-1e03-4e4e-a720-3e66645fabf5"
      },
      "outputs": [],
      "source": [
        "llama_qa = LLaMAQA()\n",
        "\n",
        "query = \"What does 'Florida' mean in Spanish?\"\n",
        "response = llama_qa.ask_question(query)\n",
        "print(\"Q:\", query)\n",
        "print(\"A:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2871910-1670-44f2-b27b-ffbd3a9d6241",
      "metadata": {
        "id": "d2871910-1670-44f2-b27b-ffbd3a9d6241"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<style>\n",
        "    table {margin-left: 0 !important;}\n",
        "    p {font-family: verdana;}\n",
        "    li {font-family: verdana;}\n",
        "    div {font-size: 10pt;}\n",
        "</style>\n",
        "<!-- Display markdown tables left oriented in this notebook. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7da4bd1-05a8-4759-b2dc-a0b4bc934886",
      "metadata": {
        "id": "c7da4bd1-05a8-4759-b2dc-a0b4bc934886"
      },
      "source": [
        "***\n",
        "***"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}